{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## NanoGPT with Attention Matrix Visualized"
      ],
      "metadata": {
        "id": "Fzz0bUNSVcWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "context_length = 256 # 256 characters of context (to predict 257th)\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-3 # brought down LR since NN is bigger\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 6 # every head is 384/6 = 64 dimensional\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "\n",
        "! wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# averages loss over multiple losses to have a smoother and less noisy loss over time\n",
        "@torch.no_grad() # tell pytorch we have no intetion to do backprop so torch can be more memory efficient when running this fxn\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    ix = torch.randint(len(data) - context_length, (batch_size,))\n",
        "    x = torch.stack([data[i:i+context_length] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+context_length+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # 90/10 train/val split\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n"
      ],
      "metadata": {
        "id": "Pz0plV34H6rh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get unique chars in training data\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# map chars to integers (idx)\n",
        "stoi = { ch:i for i,ch in enumerate(chars) } # string to int\n",
        "itos = { i:ch for i,ch in enumerate(chars) } # int to string\n",
        "encode = lambda s: [stoi[c] for c in s] # string -> list of ints\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # list of ints -> string\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(context_length, context_length))) # add a persistent buffer (self.tril) to the nn.module which will not be updated during backpropagation\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)]) # multiple heads in parallel\n",
        "        self.proj = nn.Linear(n_embd, n_embd) # residual connection\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x): # run heads in parallel and concatenate them together in the channel dimension\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1) # output of self-attention\n",
        "        out = self.dropout(self.proj(out)) # apply projection (projection back into residual pathway). Projection is just a linear transformation of the output of self-attention\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd), # inner-layer has multiplier of 4 for more neurons that does computing (just performed best empirically according to \"attention is all you need\" paper)\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd), # projection layer going back into residual pathway\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension (32)\n",
        "        # n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head # head size = 8\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # pre-norm formulation, ln1 and ln2 are applied before the sa and ffwd\n",
        "        x = x + self.sa(self.ln1(x)) # \"data communication\": sa = self-attention\n",
        "        x = x + self.ffwd(self.ln2(x)) # \"data computation\": think on that data (all the tokens) independently\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) # n_embed=32 dimensional embedding\n",
        "        self.position_embedding_table = nn.Embedding(context_length, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)]) # n_layer amount of blocks\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,n_embd)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            # inference mode\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last context_length tokens (because positional emedding is only size context_length)\n",
        "            idx_cond = idx[:, -context_length:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
      ],
      "metadata": {
        "id": "fjjvMifYZf7x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69e01e26-9003-484f-94aa-3f5b3b271d2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-10 21:51:12--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-03-10 21:51:13 (40.0 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n",
            "10.788929 M parameters\n",
            "step 0: train loss 4.2849, val loss 4.2823\n",
            "step 500: train loss 1.9703, val loss 2.0866\n",
            "step 1000: train loss 1.5154, val loss 1.7258\n",
            "step 1500: train loss 1.3800, val loss 1.6035\n",
            "step 2000: train loss 1.2776, val loss 1.5499\n",
            "step 2500: train loss 1.2050, val loss 1.5212\n",
            "step 3000: train loss 1.1403, val loss 1.5140\n",
            "step 3500: train loss 1.0849, val loss 1.5151\n",
            "step 4000: train loss 1.0303, val loss 1.5075\n",
            "step 4500: train loss 0.9691, val loss 1.5275\n",
            "step 4999: train loss 0.9132, val loss 1.5715\n",
            "\n",
            "BUCKINGHAM:\n",
            "Good mother, tell him that pake him my Catiff;\n",
            "For once against another will I wish\n",
            "But borning Richard, nor Rosaline.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "The same of the senaty of her babes:\n",
            "By heaven I heard of the time of right;\n",
            "My dear dies bleeds the other end of my head\n",
            "Which stain'd with declinessful in blaspoy\n",
            "And down and lightallowshful perjurament.\n",
            "Prepost, patiencing overty\n",
            "In three cypt leisure weight of her helms\n",
            "Conspirguise havined a heaven in day.\n",
            "The navel of his right more state unto\n",
            "Of his chantments and odd slemn mothers,\n",
            "When he called, left not rash of me, nothing dis\n",
            "In peace to blood his sake: and his nothing broils sin,\n",
            "When he, sour to hear me but all weight,\n",
            "By many postered of, but will and watch at all\n",
            "Of wild-seeming chance firements was your flesh tears?\n",
            "For who swould for sovereigns bear my course?\n",
            "\n",
            "QUEEN MARGARET:\n",
            "Have and but thou answer these fortunes are the curmourage?\n",
            "Ratchets against Cain gentle welcome;\n",
            "That raises Romeo is to the foul Tyrrel's duke:\n",
            "And, Aumerle, bear her to Calarence, his name!\n",
            "\n",
            "RIVERS:\n",
            "That's son that I slain, my will make, for strive Hasting him.\n",
            "\n",
            "QUEEN MARGARET:\n",
            "Beside him to the world at my knowld!\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "His guard\n",
            "The rest, were stands fellown to Richmond,\n",
            "And some high all buzzles reward by my childrence;\n",
            "And, not Richard is constalte, to rage,\n",
            "And you an Edward comfort of York.\n",
            "\n",
            "KING EDWARD IV:\n",
            "Now, mighty mean, for that I am do perform'd.\n",
            "\n",
            "QUEEN MARGARET:\n",
            "See thou the king with a which if villain,\n",
            "We will right thee here there; and if hew,\n",
            "We looked upon false coffice to the charge\n",
            "From the king appeace: they do not him with all;\n",
            "And hither he heard them speak upon our eyes.\n",
            "Thy father shall hear it, before thee rest to Creterpet,\n",
            "Noble Duke and with Paul, follows and tact treaty.\n",
            "\n",
            "CLIFFORD:\n",
            "Peace of Plantagenet, shall I see for thy cousin;\n",
            "But substitude ornament from waro\n",
            "A powerful praged fearle as soft sorrow and I:\n",
            "My tongue castliers for unprofanation of thyself:\n",
            "Or hence be the conspired\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fdxiuZJeto6u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}